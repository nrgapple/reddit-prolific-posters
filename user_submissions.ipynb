{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out the users that post the most in a set of subreddits\n",
    "\n",
    "edit the `sub_names` var to add/remove subreddits from the calculation\n",
    "\n",
    "This one is for the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "##### vars #####\n",
    "show = 20\n",
    "time_filter = \"month\"\n",
    "sub_names = ['politics', 'all', 'news', 'interestingasfuck', 'whitePeopleTwitter', 'worldnews', 'todayilearned', 'BlackPeopleTwitter', 'PoliticalHumor', 'MadeMeSmile', 'memes', 'pics', 'antiwork', 'PublicFreakout', 'funny', 'askreddit', 'maybemaybemaybe', 'oddlysatisfying', 'nextfuckinglevel', 'science']\n",
    "################\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv('client_id'),\n",
    "    client_secret=os.getenv('client_secret'),\n",
    "    user_agent=\"submissions comments\",\n",
    "    username=os.getenv('username'),\n",
    ")\n",
    "\n",
    "def serialize_reddit(s): \n",
    "    return {\n",
    "        \"author\": s.author,\n",
    "        \"created_utc\": s.created_utc,\n",
    "    }\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "submissions = list(flatten(reddit.subreddit(s).top(time_filter=time_filter,limit=200) for s in sub_names))\n",
    "df = pd.DataFrame([serialize_reddit(s) for s in submissions])\n",
    "df['created_utc'] = pd.to_datetime(df.created_utc.values, unit='s', utc=True)\n",
    "\n",
    "print(sub_names)\n",
    "\n",
    "ndf = df.query('author != \"None\"')\n",
    "print('Total submissions:', len(ndf))\n",
    "\n",
    "print('Number of unique authors:', ndf.author.nunique())\n",
    "\n",
    "csum = ndf.author.value_counts(1).cumsum()\n",
    "\n",
    "bins = pd.cut(csum, [0., 0.25, 0.5, 0.75, 1.], labels=['25%','50%','75%','100%'])\n",
    "\n",
    "print('\\nQuartiles:')\n",
    "print(csum.groupby(bins).size())\n",
    "\n",
    "print(f'\\nTop {show} authors:')\n",
    "topa = pd.concat([ndf.author.value_counts().head(show), ndf.author.value_counts(1).head(show)*100],axis=1)\n",
    "topa.index.name = 'Author'\n",
    "topa.columns = ('Count', 'Percent')\n",
    "print(topa)\n",
    "\n",
    "print(f'\\nPercent sum {ndf.author.value_counts(1).head(show).sum() * 100}')\n",
    "print('\\n======\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out the users that post the most in a set of subreddits\n",
    "\n",
    "edit the `sub_names` var to add/remove subreddits from the calculation\n",
    "\n",
    "This one is for the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['politics', 'all', 'news', 'interestingasfuck', 'whitePeopleTwitter', 'worldnews', 'todayilearned', 'BlackPeopleTwitter', 'PoliticalHumor', 'MadeMeSmile', 'memes', 'pics', 'antiwork', 'PublicFreakout', 'funny', 'askreddit', 'maybemaybemaybe', 'oddlysatisfying', 'nextfuckinglevel', 'science']\n",
      "Total submissions: 4000\n",
      "Number of unique authors: 2689\n",
      "\n",
      "Quartiles:\n",
      "author\n",
      "25%     152\n",
      "50%     593\n",
      "75%     972\n",
      "100%    971\n",
      "Name: author, dtype: int64\n",
      "\n",
      "Top 20 authors:\n",
      "                      Count   Percent\n",
      "Author                               \n",
      "DaFunkJunkie             63  1.620787\n",
      "Wagamaga                 29  0.746077\n",
      "PeasKhichra              20  0.514536\n",
      "dilettantedebrah         19  0.488809\n",
      "MistWeaver80             16  0.411629\n",
      "Swerwin                  16  0.411629\n",
      "dobbyisafreepup          15  0.385902\n",
      "ohnoh18                  14  0.360175\n",
      "GroundbreakingSet187     14  0.360175\n",
      "Boojibs                  13  0.334448\n",
      "My_Memes_Will_Cure_U     13  0.334448\n",
      "thebelsnickle1991        13  0.334448\n",
      "rustoo                   13  0.334448\n",
      "smurfyjenkins            12  0.308721\n",
      "kevinowdziej             12  0.308721\n",
      "killHACKS                12  0.308721\n",
      "_Xyreo_                  11  0.282995\n",
      "Thryloz                  11  0.282995\n",
      "colorfulsoul_            10  0.257268\n",
      "pietradolce              10  0.257268\n",
      "\n",
      "Percent sum 8.644198610753797\n",
      "\n",
      "======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "##### vars #####\n",
    "show = 20\n",
    "time_filter = \"year\"\n",
    "sub_names = ['politics', 'all', 'news', 'interestingasfuck', 'whitePeopleTwitter', 'worldnews', 'todayilearned', 'BlackPeopleTwitter', 'PoliticalHumor', 'MadeMeSmile', 'memes', 'pics', 'antiwork', 'PublicFreakout', 'funny', 'askreddit', 'maybemaybemaybe', 'oddlysatisfying', 'nextfuckinglevel', 'science']\n",
    "################\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv('client_id'),\n",
    "    client_secret=os.getenv('client_secret'),\n",
    "    user_agent=\"submissions comments\",\n",
    "    username=os.getenv('username'),\n",
    ")\n",
    "\n",
    "def serialize_reddit(s): \n",
    "    return {\n",
    "        \"author\": s.author,\n",
    "        \"created_utc\": s.created_utc,\n",
    "    }\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "submissions = list(flatten(reddit.subreddit(s).top(time_filter=time_filter,limit=200) for s in sub_names))\n",
    "df = pd.DataFrame([serialize_reddit(s) for s in submissions])\n",
    "df['created_utc'] = pd.to_datetime(df.created_utc.values, unit='s', utc=True)\n",
    "\n",
    "print(sub_names)\n",
    "\n",
    "ndf = df.query('author != \"None\"')\n",
    "print('Total submissions:', len(ndf))\n",
    "\n",
    "print('Number of unique authors:', ndf.author.nunique())\n",
    "\n",
    "csum = ndf.author.value_counts(1).cumsum()\n",
    "\n",
    "bins = pd.cut(csum, [0., 0.25, 0.5, 0.75, 1.], labels=['25%','50%','75%','100%'])\n",
    "\n",
    "print('\\nQuartiles:')\n",
    "print(csum.groupby(bins).size())\n",
    "\n",
    "print(f'\\nTop {show} authors:')\n",
    "topa = pd.concat([ndf.author.value_counts().head(show), ndf.author.value_counts(1).head(show)*100],axis=1)\n",
    "topa.index.name = 'Author'\n",
    "topa.columns = ('Count', 'Percent')\n",
    "print(topa)\n",
    "\n",
    "print(f'\\nPercent sum {ndf.author.value_counts(1).head(show).sum() * 100}')\n",
    "print('\\n======\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('reddit-users-block-Rxij7APa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d7ecf6afba16f901927fc1162f07b9a8a7420dc9fcba892830f880204cb1b7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
